// Global default params
params {
    experimentId = 'EXPERIMENT_ID'
    config = 'CONFIG_FILE'
    parca_cpus = PARCA_CPUS
    publishDir = 'PUBLISH_DIR'
    container_image = 'IMAGE_NAME'
}

trace {
    enabled = true
    fields = 'name,native_id,status,submit,start,complete,duration,realtime,exit,realtime,%cpu,%mem,rss,peak_rss,error_action,attempt,cpu_model,workdir'
}

profiles {
    gcloud {
        // Retry once with more RAM if OOM
        process.memory = { 4.GB * task.attempt }
        // Using single core is slightly slower but much cheaper
        process.cpus = 1
        process.executor = 'google-batch'
        process.container = params.container_image
        // Necessary otherwise symlinks to other files in bucket can break
        process.containerOptions = '--volume /mnt/disks/BUCKET:/mnt/disks/BUCKET'
        process {
            withLabel: parca {
                cpus = params.parca_cpus
                memory = params.parca_cpus * 2.GB
            }
        }
        process.errorStrategy = {
            // Codes: 137 (out-of-memory), 50001 - 50006 (Google Batch task fail:
            // https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes)
            (((task.exitStatus == 137) || (task.exitStatus >= 50001 && task.exitStatus <= 50006))
            && (task.attempt <= process.maxRetries)) ? 'retry' : 'ignore' }
        // For this script to work on a Compute Engine VM, you must
        // - Set default Compute Engine region and zone for your project
        // - Set access scope to "Allow full access to all Cloud APIs" when
        //   creating VM
        // - Run gcloud init in VM
        google.project = {
            def project_proc = "gcloud config get project".execute()
            project_proc.waitFor()
            return project_proc.text.trim()
        }()
        google.location = {
            def location_proc = "gcloud config get compute/region".execute()
            location_proc.waitFor()
            return location_proc.text.trim()
        }()
        google.batch.spot = true
        google.batch.usePrivateAddress = true
        google.batch.network = 'global/networks/default'
        google.batch.subnetwork = "regions/${google.location}/subnetworks/default"
        docker.enabled = true
        params.projectRoot = '/vEcoli'
        process.maxRetries = 1
        // Check Google Cloud latest spot pricing / performance
        process.machineType = { 
            def cpus = task.cpus
            def powerOf2 = 1
            while (powerOf2 < cpus && powerOf2 < 64) {
                powerOf2 *= 2
            }
            return "t2d-standard-${powerOf2}"
        }
        workflow.failOnIgnore = true
    }
    sherlock {
        process.memory = {
            if ( task.exitStatus in [137, 140] ) {
                4.GB * task.attempt
            } else {
                4.GB
            }
        }
        process.errorStrategy = {
            // Codes: 140 (SLURM job limits), 143 (SLURM preemption)
            // Default value for exitStatus is max integer value, this
            // is a catch-all for errors that leave no exit code
            ((task.exitStatus in [140, 143, Integer.MAX_VALUE])
            && (task.attempt <= process.maxRetries)) ? 'retry' : 'ignore' }
        // Using single core is slightly slower but can have shorter
        // queue times and is less damaging to future job priority
        process.cpus = 1
        process.executor = 'slurm'
        process.queue = 'mcovert,owners'
        process.container = params.container_image
        apptainer.enabled = true
        process {
            // Run analyses, create variants, and run ParCa locally with
            // the job used to launch workflow to avoid long queue times
            withLabel: short {
                executor = 'local'
            },
            // ParCa 4 CPUs in ~15 min, 1 CPU in ~30 min, not too bad
            withLabel: parca {
                executor = 'local'
                cpus = 1
                memory = 2.GB
            }
        }
        process.time = {
            if ( task.exitStatus == 140 ) {
                1.h * task.attempt
            } else {
                1.h
            }
        }
        process.maxRetries = 3
        params.projectRoot = "${launchDir}"
        // Avoid getting queue status too frequently (can cause job status mixups)
        executor.queueStatInterval = '2 min'
        // Check for terminated jobs and submit new ones fairly frequently
        // to minimize downtime between dependent jobs
        executor.pollInterval = '30 sec'
        // Retry all jobs that fail to submit (different from fail during runtime)
        executor.submit.retry.reason = '.*'
        // Retry failed submissions with delay longer than the time to
        // get latest queue status (avoid job status mixups)
        executor.retry.delay = '3 min'
        executor.retry.maxDelay = '5 min'
        // Throttle submission rate to avoid overwhelming scheduler
        executor.submitRateLimit = '20/min'
        // Give NFS time to update and sync before raising errors
        executor.exitReadTimeout = '10 min'
        workflow.failOnIgnore = true
    }
    standard {
        process.executor = 'local'
        params.projectRoot = "${launchDir}"
        workflow.failOnIgnore = true
        process.errorStrategy = 'ignore'
        process {
            withLabel: parca {
                cpus = params.parca_cpus
                memory = params.parca_cpus * 2.GB
            }
        }
    }
}
