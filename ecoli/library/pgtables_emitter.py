import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import Any, Callable, Iterable, Mapping
from datetime import datetime, timedelta
import atexit
import subprocess

import asyncpg
import psycopg
import orjson
import uvloop
from vivarium.core.emitter import Emitter
from vivarium.core.serialize import make_fallback_serializer_function

asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

INDEXES_TO_CREATE = [
    'experiment_id', 'variant', 'seed', 'generation', 'agent_id']
"""Default indexes to create for ``configuration`` table."""


async def create_hypertable(conn_args):
    con = await asyncpg.connect(**conn_args)
    await con.execute('CREATE TABLE IF NOT EXISTS history_colnames ('
        'seqname INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY, '
        'fieldname TEXT UNIQUE)')
    await con.execute('CREATE TABLE IF NOT EXISTS history ('
        'exp_id_time TIMESTAMPTZ, agent_id TEXT)')
    await con.execute(
        "SELECT create_hypertable('history', "
        "by_range('exp_id_time', INTERVAL '400 seconds'), "
        "if_not_exists => TRUE)")
    await con.execute('CREATE INDEX IF NOT EXISTS agent_exp_id_idx '
        'ON history (agent_id, exp_id_time)')
    await con.execute(
        "ALTER TABLE history SET (timescaledb.compress = TRUE, "
        "timescaledb.compress_orderby = 'exp_id_time',"
        "timescaledb.compress_segmentby = 'agent_id')")
    await con.execute(
        "SELECT add_compression_policy('history', "
        "compress_created_before => INTERVAL '5 minutes', "
        "if_not_exists => TRUE)")
    await con.close()


async def add_new_fields(conn_args: dict[str, Any],
                         field_types: dict[str, str]) -> dict[str, str]:
    """
    Create new fields in a table.

    Args:
        conn_args: Keyword arguments for :py:func:`asyncpg.connect`
        table_id: Name of table to create new fields for
        field_types: Mapping of new field names to types (from
            :py:func:`~.get_pg_type`)

    Returns:
        Mapping of fields to placeholder names after any new fields added.
    """
    con = await asyncpg.connect(**conn_args)
    # fieldname has UNIQUE constraint and should raise a conflict for
    # duplicates. We tell PostgreSQL to skip inserting these duplicates.
    cmd_prefix = 'INSERT INTO history_colnames (fieldname) VALUES '
    col_values = []
    for i in range(len(field_types)):
        col_values.append(f'(${i+1})')
    col_values = ", ".join(col_values)
    cmd = ''.join([cmd_prefix, col_values, ' ON CONFLICT DO NOTHING'])
    await con.execute(cmd, *field_types)
    res = await con.fetch("SELECT (fieldname, seqname) FROM history_colnames")
    field_to_placeholder = dict(i[0] for i in res)
    col_str = [f'"{field_to_placeholder[k]}" {v}' for k, v in field_types.items()]
    # If any new column names are duplicates, nothing should happen.
    add_cols = ', '.join([f'ADD COLUMN IF NOT EXISTS {i}' for i in col_str])
    await con.execute(f'ALTER TABLE history {add_cols}')
    await con.close()
    return field_to_placeholder


async def emit_data(conn_args: dict[str, Any],
                    filename: str, columns: list[str]):
    """
    Called by :py:func:`~.executor_proc` to insert data.

    Args:
        conn_args: Keyword arguments for :py:func:`asyncpg.connect`
        cell_id: Unique identifier for data emitted by one simulated cell
        inserts: Tuples ``(table_id, values, columns)``
    """
    conn = await asyncpg.connect(**conn_args)
    with open(filename, 'rb') as f:
        await conn.copy_to_table('history', source=f, columns=columns,
                                 format='text', delimiter='\t', null='"null"')
    await conn.close()


_FLAG_FIRST = object()

def flatten_dict(d: dict):
    """
    Flatten nested dictionary down to key-value pairs where each key
    concatenates all the keys needed to reach the corresponding value
    in the input into comma-separated string. Prunes empty dicts and lists.
    """
    results = []
    result_types = []

    def visit(subdict, results, result_types, partialKey):
        for k, v in subdict.items():
            newKey = k if partialKey==_FLAG_FIRST else f'{partialKey}__{k}'
            if isinstance(v, Mapping):
                visit(v, results, result_types, newKey)
            else:
                # Skip empty lists and None values
                if v is None or (isinstance(v, list) and len(v) == 0):
                    continue
                pg_type = get_pg_type(v, newKey)
                result_types.append((newKey, pg_type))
                results.append((newKey, v))

    visit(d, results, result_types, _FLAG_FIRST)
    return dict(results), dict(result_types)


def get_pg_type(py_val: Any, fieldname: str):
    """
    Return PostgreSQL type to figure out what column type to create
    for an emit value.
    """
    if isinstance(py_val, bool):
        return 'boolean'
    elif isinstance(py_val, int):
        return 'bigint'
    elif isinstance(py_val, float):
        return 'double precision'
    if isinstance(py_val, str):
        return 'text'
    elif isinstance(py_val, list):
        inner_pg_type = get_pg_type(py_val[0], fieldname)
        return  inner_pg_type + '[]'
    raise TypeError(f'({fieldname}) contains unsupported type.')


async def emit_config(conn_args, d, fallback_serializer):
    metadata = d.pop('metadata', None)
    if metadata is not None:
        d = {**metadata, **d}
    d['generation'] = len(d['agent_id'])
    # TODO: These keys need to be added
    d['variant'] = None
    d['seed'] = None
    conn = await asyncpg.connect(**conn_args)
    await conn.execute('CREATE TABLE IF NOT EXISTS configuration '
        '(cell_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY, '
        'config JSONB)')
    for index in INDEXES_TO_CREATE:
        await conn.execute(f"CREATE INDEX IF NOT EXISTS {index}_idx ON "
            f"configuration((config->>'{index}'))")
    # PostgreSQL JSONB is binary string with "1" byte prepended
    await conn.set_type_codec(
        "jsonb",
        encoder=lambda data: b"\x01" + orjson.dumps(
            data, option=orjson.OPT_SERIALIZE_NUMPY, 
            default=fallback_serializer),
        decoder=lambda data: orjson.loads(data[1:]),
        schema="pg_catalog",
        format="binary"
    )
    # Inserting the configuration should generate a unique cell ID
    # that we can use to set the year for the cell_id_time column
    # used to uniquely identify each data point in the history table
    insert_cmd = ("INSERT INTO configuration (config)"
        " VALUES ($1) RETURNING cell_id")
    # Python years cannot be year 0, so increment by 1
    return await conn.fetchval(insert_cmd, d) + 1


class PgtablesEmitter(Emitter):
    """
    Emit data to a PostgreSQL database. Creates a separate OS thread
    to handle the asynchronous insert operations.
    """

    def __init__(self, config: dict[str, Any]) -> None:
        """
        Pull connection arguments from ``config`` and start separate OS
        process for database inserts.

        Args:
            config: Must include ``experiment_id`` key. Can include keys for
                ``host``, ``port``, ``user``, ``database``, and ``password``
                to use as keyword arguments for :py:func:`asyncpg.connect`. 
        """
        self.experiment_id = config.get('experiment_id')
        # Collect connection arguments
        self.connection_args = {
            'host': config.get('host', None),
            'port': config.get('port', None),
            'user': config.get('user', None),
            'database': config.get('database', 'tsdb'),
            'password': config.get('password', None)
        }
        self.field_to_placeholder = {}
        self.fallback_serializer = make_fallback_serializer_function()
        self.temp_file = open(f'{self.experiment_id}_temp.tsv', 'a+')
        atexit.register(self._push_to_db)

    def _push_to_db(self):
        self.temp_file.close()
        columns = ','.join(['exp_id_time', 'agent_id', *(str(v) for v in self.field_to_placeholder.values())])
        with open(f'{self.experiment_id}_temp.tsv', 'rb') as buf:
            with psycopg.connect(**self.connection_args) as conn:
                with conn.cursor() as cursor:
                    with cursor.copy(f'COPY history ({columns}) FROM STDIN (NULL '"NULL"')') as copy:
                        copy.write(buf)
        subprocess.run(['rm', f'{self.experiment_id}_temp.tsv'], check=True)

    def emit(self, data: dict[str, Any]):
        data = orjson.loads(orjson.dumps(
            data, option=orjson.OPT_SERIALIZE_NUMPY, 
            default=self.fallback_serializer))
        # Config will always be first emit
        if data['table'] == 'configuration':
            cell_id = asyncio.run(emit_config(self.connection_args,
                data['data'], self.fallback_serializer))
            self.base_time = datetime(cell_id, 1, 1)
            return
        time = data['data']['time']
        for agent_id, agent_data in data['data']['agents'].items():
            agent_data['generation'] = len(agent_id)
            agent_data, data_types = flatten_dict(agent_data)
            # New columns needed when new Stores are created
            new_cols = set(agent_data) - set(self.field_to_placeholder)
            if len(new_cols) > 0:
                if len(self.field_to_placeholder) != 0:
                    self._push_to_db()
                # If starting from a fresh DB, we need to add new cols so
                # take this opportunity to create TimescaleDB hypertable
                asyncio.run(create_hypertable(self.connection_args))
                new_col_types = {k: data_types[k] for k in new_cols}
                self.field_to_placeholder = asyncio.run(
                    add_new_fields(self.connection_args, new_col_types))
            cell_id_time = self.base_time + timedelta(seconds=time)
            self.temp_file.write(str(cell_id_time))
            self.temp_file.write('\t')
            self.temp_file.write(agent_id)
            for k in self.field_to_placeholder:
                self.temp_file.write('\t')
                py_str = orjson.dumps(agent_data.get(k, 'null')).decode('utf-8')
                self.temp_file.write(py_str.replace('[', '{').replace(']', '}'))
            self.temp_file.write('\n')
